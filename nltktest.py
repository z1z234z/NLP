from nltk.tokenize import word_tokenize
words = word_tokenize('And now for something completely different')
print(words)